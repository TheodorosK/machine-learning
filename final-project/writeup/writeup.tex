\documentclass[journal]{IEEEtran}

\input{../../utils/header_final.tex}

\begin{document}

\title{Facial Keypoints Detection}
\author{Will Clark and Matthew DeLio\\
University of Chicago Booth School of Business\\
\textsf{\{will.clark,mdelio\}@chicagobooth.edu}}

% The paper headers
\markboth{Machine Learning (BUS 41201) Final Project}
{Machine Learning (BUS 41204) Final Project}

\maketitle

\section{Summary}

\section{Modelling}

\subsection{Facial Keypoints Introduction}

The goal of this paper is to predict the $(x,y)$ coordinates of 15 facial keypoints for a given set of 96-by-96 grayscale images. The keypoints are:
\begin{itemize}
\item Left and right eye center (2)
\item Left and right eye inner and outer corners (4)
\item Left and right eyebrow inner and outer ends (4)
\item Nose tip (1)
\item Mouth left and right corner (2)
\item Mouth top and bottom lip center (2)
\end{itemize}
A random sample of images and the associated keypoints are shown for reference in \cref{fig:random_faces}. Note that of the displayed faces, only two have the full set of keypoints plotted. This is a feature of the data, namely, that most faces are missing at least multiple keypoints. Accordingly, we predict two values for each keypoint: (1) whether it is present in an image or not, and (2) what its $(x,y)$ coordinates would be if it were present.

There are 7049 images in the testing set provided by Kaggle. We subset the data so that 70 percent of images are for training a model and the other 30 percent are for validation. There is also a true test set of 1783 images for which we only have the image data but not the keypoint data. This is the set on which the Kaggle contest is judged.

The criterion for model accuracy (and evaluation in the Kaggle contest) is the root means square error of all keypoint coordinates:
\[ \text{RMSE}=\sqrt{\frac{1}{n} \sum_{i=1}^{n} \left( y_i - \hat{y}_i\right)^2} \]

\begin{figure}[!ht]
  \centering
  \caption{Random Sub-Sample of Faces and Keypoints}
  \includegraphics[scale=.6]{random_faces.pdf}
  \label{fig:random_faces}
\end{figure}

\subsection{Building a Model (Tools \& Software)}

The main tool we use in this paper is a multilayer convolutional neural network. We built and trained the model in Python using the Theano and Lasagne libraries.\footnote{Lasagne is "a lightweight library to build and train neural networks in Theano." See \url{https://github.com/Lasagne/Lasagne} for details.} One advantage of this technology stack is that computation can be carried out on a graphics card (GPU) which can significantly improve calculation speed.

To take advantage of Theano's GPU capability, we set up our computing environment using a g2.2xlarge EC2 instance from Amazon Web Services. Doing so gave us access to an NVIDIA GPUs with 1536 CUDA cores that performed the bulk of the calculation. In our case, the performance improvement from utilizing the g2.2xlarge instance (compared to running locally) was approximately 40x.

\subsection{Initial Models}
What we tried in a nutshell:
\begin{enumerate}
\item 3-level - 1 convolutional, 2 dense
\item 6-level - 3 convolutional, 3 dense
\item 6-level with dropout
\item Batch-size changes - size positively correlated with minimum loss-floor.
\item Varying learning rate/momentum: static, and dynamic (as prescribed by \cite{lasagnenesterov} to balance increasing momentum must decrease learning rate)
\item Wider layers
\item More/Less Dropout
\item Image Preprocessing
\item Amputated w/RF
\end{enumerate}

\subsection{Individual Feature Models}
Should outline how we came up with the idea of training different models for each feature (set of features).


Dropout, l1/l2 regularization, and momentum/learning rate.


Using the advice given in Andrew Ng's paper on regularization \cite{ng2004feature}, by acknowledging the fact that we have a dimensionally large input data-set, we choose to apply L1 regularization. <Talk about how we do see a more gradual, but smoother drop in the loss function>

Should plot some validation loss (rmse).

Include any citations for the blog we ran across with \cite{dnouri}

\subsection{Missing Feature Model}
Since the data contain many missing features caused by partially obscured images, our resulting model must also predict the presence of features.  To predict this we turn to a slightly modified version with a sigmoid non-linearity on the output layer and the binary cross-entropy loss function defined in \cref{eq:bin_cross}.

\[\label{eq:bin_cross}
 L = -target \log(p) - (1 - target) \log(1 - p)
\]
where \texttt{target} indicates whether the feature is actually missing ($\in \{0,1\}$) and \texttt{p} is the predicted probability of the feature being missing

For many of the features, the neural network does a good job separating the two classes and producing probabilities that make logical sense.  The ones that it doesn't separate well, like \texttt{Left Eye Center}, \texttt{Right Eye Center} and \texttt{Mouth Center Bottom Lip}, have few missing data-point exemplars in the training data (9, 9, and 23 respectively out of 4934 samples).

\begin{figure}[!ht]
  \centering
  \caption{Boxplots for Missing Eye Features}
  \includegraphics[scale=.5]{logistic_boxplots_eye.pdf}
  \label{fig:logistic_boxplots_eye}
\end{figure}

% We can probably drop these next two, they don't add much to the discussion...maybe we can just mention they they look identical (throw them in an appendix)
\begin{figure}[!ht]
  \centering
  \caption{Boxplots for Missing Eyebrow Features}
  \includegraphics[scale=.5]{logistic_boxplots_eyebrow.pdf}
  \label{fig:logistic_boxplots_eyebrow}
\end{figure}

\begin{figure}[!ht]
  \centering
  \caption{Boxplots for Missing Mouth Features}
  \includegraphics[scale=.5]{logistic_boxplots_mouth.pdf}
  \label{fig:logistic_boxplots_mouth}
\end{figure}

% \begin{figure}[!ht]
%   \centering
%   \caption{Boxplots for Missing Nose Feature}
%   \includegraphics[scale=.5]{logistic_boxplots_nose.pdf}
%   \label{fig:logistic_boxplots_nose}
% \end{figure}

With the output trained, we next turn to determining the optimal cutoff.  Using the R package \texttt{OptimalCutpoints} we choose to maximize the accuracy area \cite{lewis2008use,greiner1995two,greiner1996two} which is defined in \cref{eq:roc_aa}.

\[\label{eq:roc_aa}
AA(c)=\frac{TP(c)TN(c)}{(TP(c)+FN(c))(FP(c)+TN(c))}
\]
where $TP$ = True Positives, $TN$ = True Negatives, $FN$ = False Negatives, \& $FP$ = False Positives

Looking over our boxplots, we see there are generally two categories of features: those that are missing often and those that are not.  For the ones that are missing often, we have a relatively good separability between the predicted probabilities.  For those that are not, the separately is questionable at best.  Turning to the ROC plot for a feature that is missing quite often, \texttt{Left Eye Inner Corner}, (see \cref{fig:roc_left_eye_inner_corner}) we see there is a lot of area under the ROC curve and the optimal cutoff point of 0.5 seems to put us right on the upper-left edge of the curve.  Next, looking at \texttt{Left Eye Center}, a feature with few missing exemplars, (see \cref{fig:roc_left_eye_center}) we see less area and a clear trade-off between specificity and sensitivity.  Again, a value of 0.5 seems to put us on the upper-left edge of the curve.

It turns out that, given the optimization condition, the best cutoff for all of these features is 0.5 (see \cref{tab:logistic_cutoff_table}\footnote{Note: that the \texttt{Nose Tip} feature is present in all training and validation images.}).  In each of the cases, we end up with quite a large sensitivity and a reasonable specificity.

\begin{figure}[!htb]
  \centering
  \caption{ROC Curve for Left Eye Inner Corner}
  \includegraphics[scale=.5]{roc_left_eye_inner_corner.pdf}
  \label{fig:roc_left_eye_inner_corner}
\end{figure}

\begin{figure}[!htb]
  \centering
  \caption{ROC Curve For Left Eye Center Feature}
  \includegraphics[scale=.5]{roc_left_eye_center.pdf}
  \label{fig:roc_left_eye_center}
\end{figure}


\input{logistic_cutoff_table.tex}

\subsection{Combined Loss Model}
Semi-failed attempt to train missing signals and coordinates simultaneously.

\subsection{Putting It All Together}

In \cref{fig:avg_face_rmse}, we show an ``average'' face, i.e., one that is a composite of all images in our data set. On the image is the average location of each keypoint (marked with a ``+'' sign), and the shaded circles show the average deviation of the predicted keypoints from the actual keypoints. Colors correspond to the model used for each set of keypoints (i.e. eye corners were all predicted with one model, so they are all depicted in yellow).

We can consider the accuracy of our model in two different ways: (1) the RMSE of each keypoint coordinate, or (2) the Euclidean distance of the predicted keypoint $(x,y)$ coordinate from the actual $(x,y)$ coordinate (the latter are represented by the circles in \cref{fig:avg_face_rmse}. 

\begin{figure}[!htb]
  \centering
  \caption{Average Face with Estimated Keypoints}
  \includegraphics[scale=.5]{avg_face_rmse.pdf}
  \label{fig:avg_face_rmse}
\end{figure}

\begin{figure}[!htb]
  \centering
  \caption{Most Accurate Predictions (Average RMSE = 1.768764)}
  \includegraphics[scale=.6]{best_faces.pdf}
  \label{fig:best_faces}
\end{figure}

\begin{figure}[!htb]
  \centering
  \caption{Least Accurate Predictions (Average RMSE = 7.874467)}
  \includegraphics[scale=.6]{worst_faces.pdf}
  \label{fig:worst_faces}
\end{figure}

\section{Kaggle Submission}
Ranking?

\section{Applications}
\begin{enumerate}
\item Engagement Detection
\end{enumerate}

\medskip

\bibliographystyle{IEEEtran}
% \bibliographystyle{unsrt}
\bibliography{citations}

\end{document}

% \input{.tex}

% \begin{figure}[!htb]
%   \centering
%   \begin{subfigure}[b]{0.49\textwidth}
%     \caption{}
%     \includegraphics[width=\textwidth]{.pdf}
%     \label{fig:}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}[b]{0.49\textwidth}
%     \caption{}
%     \includegraphics[width=\textwidth]{.pdf}
%     \label{fig:}
%   \end{subfigure}
%   \caption{}
% \end{figure}

% \begin{figure}[!htb]
%   \centering
%   \caption{}
%   \includegraphics[scale=.5]{.pdf}
%   \label{fig:}
% \end{figure}