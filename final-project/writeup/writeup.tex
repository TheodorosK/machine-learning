\documentclass[journal]{IEEEtran}

\input{../../utils/header_final.tex}

\begin{document}

\title{Facial Keypoints Detection}
\author{Will Clark and Matthew DeLio\\
University of Chicago Booth School of Business\\
\textsf{\{will.clark,mdelio\}@chicagobooth.edu}}

% The paper headers
\markboth{Machine Learning (BUS 41201) Final Project}
{Machine Learning (BUS 41204) Final Project}

\maketitle

\section{Introduction}

\subsection{Problem Summary}

The goal of this paper is to predict the $(x,y)$ coordinates of 15 facial keypoints for a given set of 96-by-96 grayscale images. The keypoints are:
\begin{itemize}
\item Left and right eye center (2)
\item Left and right eye inner and outer corners (4)
\item Left and right eyebrow inner and outer ends (4)
\item Nose tip (1)
\item Mouth left and right corner (2)
\item Mouth top and bottom lip center (2)
\end{itemize}
A random sample of images and the associated keypoints are shown for reference in \cref{fig:random_faces}. Note that of the displayed faces, only two have the full set of keypoints plotted. This is a feature of the data, namely, that most faces are missing at least multiple keypoints. Accordingly, we predict two values for each keypoint: (1) whether it is present in an image or not, and (2) what its $(x,y)$ coordinates would be if it were present.

There are 7049 images in the testing set provided by Kaggle. We subset the data so that 70 percent of images are for training a model and the other 30 percent are for validation. There is also a true test set of 1783 images for which we only have the image data but not the keypoint data. This is the set on which the Kaggle contest is judged.

The criterion for model accuracy (and evaluation in the Kaggle contest) is the root means square error of all keypoint coordinates:
\[ \text{RMSE}=\sqrt{\frac{1}{n} \sum_{i=1}^{n} \left( y_i - \hat{y}_i\right)^2} \]

\begin{figure}[!ht]
  \centering
  \caption{Random Sub-Sample of Faces and Keypoints}
  \includegraphics[scale=.6]{random_faces.pdf}
  \label{fig:random_faces}
\end{figure}

\subsection{Data}

As discussed in \cref{intro}, most images do not contain all keypoints, which presents a problem when we are trying to train our model. One naive solution would be to throw out all faces without all keypoints, but doing so would remove 4909 images from our training data set (out of 7049 total images)--about 70 percent of our data.

An important observation is that keypoints tend to be missing in groups, so that (as an example) if an image does not have one of the eyebrow keypoints, it tends to also not have all other eyebrow keypoints. Accordingly, we bundle the keypoints into the following groups (the number of images containing all keypoints in the group are reported in parentheses):
\begin{itemize}
\item eye center: left eye center, right eye center (7033)
\item eye corner: left eye inner corner, left eye outer corner, right eye inner corner, right eye outer corner (2247)
\item eyebrow: left eyebrow inner end, left eyebrow outer end, right eyebrow inner end, right eyebrow outer end (2190)
\item mouth (bottom): mouth center bottom lip (7016)
\item mouth (excl. bottom): mouth left corner, mouth right corner, mouth center top lip (2260)
\item nose: nose tip (7049)
\end{itemize}
For each feature, then, we can group it with other features that tend to be present in the same image. Within a group, we throw out images in which all keypoints \textit{in that group} are not present, but in 22 out of 30 cases this requires us to remove less than 1 percent of the available images (in the other 8 cases, we throw out between 1 and 4 percent of available images).

Splitting the features into groups like this requires that we train six models, but each model will have more available training data than a single model required to predict all features. 

A related point is that we will also need to predict whether or not a keypoint is contained in a feature, so that we can predict an empty value for keypoints that are likely to be missing in the test set. We discuss this model more fully in \cref{missing}.

\subsection{Tools and Software}

The main tool we use in this paper is a multilayer convolutional neural network. We built and trained the model in Python using the Theano and Lasagne libraries.\footnote{Lasagne is "a lightweight library to build and train neural networks in Theano." See \url{https://github.com/Lasagne/Lasagne} for details.} One advantage of this technology stack is that computation can be carried out on a graphics card (GPU) which can significantly improve calculation speed.

To take advantage of Theano's GPU capability, we set up our computing environment using a g2.2xlarge EC2 instance from Amazon Web Services. Doing so gave us access to an NVIDIA GPUs with 1536 CUDA cores that performed the bulk of the calculation. In our case, the performance improvement from utilizing the g2.2xlarge instance (compared to running locally) was approximately 40x.

\section{Missing Feature Model}

Since the data contain many missing features caused by partially obscured images, our resulting model must also predict the presence of features.  To predict this we turn to a slightly modified version with a sigmoid non-linearity on the output layer and the binary cross-entropy loss function defined in \cref{eq:bin_cross}.

\[\label{eq:bin_cross}
 L = -target \log(p) - (1 - target) \log(1 - p)
\]
where \texttt{target} indicates whether the feature is actually missing ($\in \{0,1\}$) and \texttt{p} is the predicted probability of the feature being missing

For many of the features, the neural network does a good job separating the two classes and producing probabilities that make logical sense.  The ones that it doesn't separate well, like \texttt{Left Eye Center}, \texttt{Right Eye Center} and \texttt{Mouth Center Bottom Lip}, have few missing data-point exemplars in the training data (9, 9, and 23 respectively out of 4934 samples).

\begin{figure}[!ht]
  \centering
  \caption{Boxplots for Missing Eye Features}
  \includegraphics[scale=.5]{logistic_boxplots_eye.pdf}
  \label{fig:logistic_boxplots_eye}
\end{figure}

% We can probably drop these next two, they don't add much to the discussion...maybe we can just mention they they look identical (throw them in an appendix)
\begin{figure}[!ht]
  \centering
  \caption{Boxplots for Missing Eyebrow Features}
  \includegraphics[scale=.5]{logistic_boxplots_eyebrow.pdf}
  \label{fig:logistic_boxplots_eyebrow}
\end{figure}

\begin{figure}[!ht]
  \centering
  \caption{Boxplots for Missing Mouth Features}
  \includegraphics[scale=.5]{logistic_boxplots_mouth.pdf}
  \label{fig:logistic_boxplots_mouth}
\end{figure}

% \begin{figure}[!ht]
%   \centering
%   \caption{Boxplots for Missing Nose Feature}
%   \includegraphics[scale=.5]{logistic_boxplots_nose.pdf}
%   \label{fig:logistic_boxplots_nose}
% \end{figure}

With the output trained, we next turn to determining the optimal cutoff.  Using the R package \texttt{OptimalCutpoints} we choose to maximize the accuracy area \cite{lewis2008use,greiner1995two,greiner1996two} which is defined in \cref{eq:roc_aa}.

\[\label{eq:roc_aa}
AA(c)=\frac{TP(c)TN(c)}{(TP(c)+FN(c))(FP(c)+TN(c))}
\]
where $TP$ = True Positives, $TN$ = True Negatives, $FN$ = False Negatives, \& $FP$ = False Positives

Looking over our boxplots, we see there are generally two categories of features: those that are missing often and those that are not.  For the ones that are missing often, we have a relatively good separability between the predicted probabilities.  For those that are not, the separately is questionable at best.  Turning to the ROC plot for a feature that is missing quite often, \texttt{Left Eye Inner Corner}, (see \cref{fig:roc_left_eye_inner_corner}) we see there is a lot of area under the ROC curve and the optimal cutoff point of 0.5 seems to put us right on the upper-left edge of the curve.  Next, looking at \texttt{Left Eye Center}, a feature with few missing exemplars, (see \cref{fig:roc_left_eye_center}) we see less area and a clear trade-off between specificity and sensitivity.  Again, a value of 0.5 seems to put us on the upper-left edge of the curve.

It turns out that, given the optimization condition, the best cutoff for all of these features is 0.5 (see \cref{tab:logistic_cutoff_table}\footnote{Note: that the \texttt{Nose Tip} feature is present in all training and validation images.}).  In each of the cases, we end up with quite a large sensitivity and a reasonable specificity.

\begin{figure}[!htb]
  \centering
  \caption{ROC Curve for Left Eye Inner Corner}
  \includegraphics[scale=.5]{roc_left_eye_inner_corner.pdf}
  \label{fig:roc_left_eye_inner_corner}
\end{figure}

\begin{figure}[!htb]
  \centering
  \caption{ROC Curve For Left Eye Center Feature}
  \includegraphics[scale=.5]{roc_left_eye_center.pdf}
  \label{fig:roc_left_eye_center}
\end{figure}

\input{logistic_cutoff_table.tex}


\section{Individual Keypoint Models}

In this section we will present a subset of the models we experimented with and report performance statistics for each. We will also discuss some auxiliary modeling techniques that we investigated.

\subsection{Three Layers}

Our baseline model is a neural network with three layers: 1 convolutional layer and 2 dense layers. The hope for the convolutional layer is that it would allow the network to automatically detect low-level features of the images (i.e. edge detection). 

\subsection{Three Layers with Regularization}

We augmented our baseline model (three layers; 1 convolutional and 2 dense) with regularization. Following \cite{ng2004feature}, we focus on L1 regularization because we have a dimensionally large input data set (each image has 96x96 pixels, where each pixel is a feature). 

Compared to the model without regularization, we find that the loss function falls more gradually and more smoothly. This is because the cost function now has a second term that penalizes model complexity, so that early on in the training cycle the model is less able to fit the data. The benefit of regularization is that the model (hopefully) is better at understanding images that it has not seen before. In other words, we are hoping to move the model more towards the optimal trade-off between bias and variance.

\subsection{Six Layers}

Next, we tried increasing the number of layers to six, with 3 convolutional layers and 3 dense layers. Following \cite{benlecun2007}, the hope here is that increasing the depth of the network would increase its predictive accuracy. Additional convolutional layers should, in theory, allow the network to discover or learn more than just the low-level features of the images and hopefully increase its ability to identify specific keypoints.

\subsection{Six Layers with Regularization}

We augmented our six layer model described above to include L2 regularization as described above. Once again, the goal here is to prevent the model from over-fitting to the images that it trains on, so penalizing model complexity should increase the out-of-sample accuracy.

\subsection{Six Layers with Regularization and Dropout}

Finally, we augmented our six layer model including L2 regularization to include dropout. Similar to regularization, the goal is to choose a simpler model, but instead of adjusting the cost function we instead alter the structure of the network by deleting nodes in the intermediate layers. As with regularization, the goal of dropout is to prevent the model from over-fitting on the images it sees and increase predictive accuracy of images that it has not trained on.

\subsection{Other Modeling Choices}

Bath size changes; learning rate, momentum; wider layers, more/less dropout; image pre-processing; random forest with output layer



Should plot some validation loss (rmse).

Include any citations for the blog we ran across with \cite{dnouri}

\section{Other Models}

\subsection{Combined Loss Model}
Semi-failed attempt to train missing signals and coordinates simultaneously.

\subsection{Amputated NN with Random Forest}
Ugh

\section{Discussion}

\subsection{Best and Worst Performances}

In \cref{fig:avg_face_rmse}, we show an ``average'' face, i.e., one that is a composite of all images in our data set. On the image is the average location of each keypoint (marked with a ``+'' sign), and the shaded circles show the average deviation of the predicted keypoints from the actual keypoints. Colors correspond to the model used for each set of keypoints (i.e. eye corners were all predicted with one model, so they are all depicted in yellow).

We can consider the accuracy of our model in two different ways: the RMSE of each keypoint coordinate (shown in \cref{tab:rmse}), or the Euclidean distance of the predicted keypoint $(x,y)$ coordinate from the actual $(x,y)$ coordinate (shown in \cref{tab:rmse}; these are represented by the circles in \cref{fig:avg_face_rmse}). 

From the distances between predicted in actual keypoints in \cref{tab:radius}, we can see that the model is best at predicting eye features. The top of the list (ordered from most to least accurate) is comprised entirely of the eye-related features (eye centers, eye corners, and eyebrows). We can see in \cref{fig:avg_face_rmse} that the radii around the eye features are smaller than those of the nose and mouth features. The most accurate keypoint is the inner corner of the right eye, which has a distance between the predicted and actual keypoints of just over three pixels.

Conversely, the model is not very good at predicting where the tip of a nose is. It is the least accurate keypoint, with a mean distance between the predicted and actual value of over six pixels.

\input{rmse.tex}
\input{radius.tex}

\subsection{Model Evaluation}

One baseline to judge model performance is how well we predict relative to naive averaging of keypoints. That is, one simple algorithm would be to predict--for all faces in the data set--the average of each keypoint across all faces. Doing so yields an average RMSE (across all keypoints) of 3.600 (compared to an average RMSE of 3.34 for our neural network). Ultimately, our algorithm only performs about 3.5 percent better than naive averaging.

One feature that stands out when examining the badly mis-predicted faces in \cref{fig:worst_faces} is that the model tends to predict keypoints close to their average values regardless of how the face in a given image is oriented.

\begin{figure}[!htb]
  \centering
  \caption{Average Face with Estimated Keypoints}
  \includegraphics[scale=.5]{avg_face_rmse.pdf}
  \label{fig:avg_face_rmse}
\end{figure}

\begin{figure}[!htb]
  \centering
  \caption{Most Accurate Predictions (Average RMSE = 1.768764)}
  \includegraphics[scale=.6]{best_faces.pdf}
  \label{fig:best_faces}
\end{figure}

\begin{figure}[!htb]
  \centering
  \caption{Least Accurate Predictions (Average RMSE = 7.874467)}
  \includegraphics[scale=.6]{worst_faces.pdf}
  \label{fig:worst_faces}
\end{figure}

\section{Kaggle Submission}

\bibliographystyle{IEEEtran}
\bibliography{citations}

\end{document}

% \input{.tex}

% \begin{figure}[!htb]
%   \centering
%   \begin{subfigure}[b]{0.49\textwidth}
%     \caption{}
%     \includegraphics[width=\textwidth]{.pdf}
%     \label{fig:}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}[b]{0.49\textwidth}
%     \caption{}
%     \includegraphics[width=\textwidth]{.pdf}
%     \label{fig:}
%   \end{subfigure}
%   \caption{}
% \end{figure}

% \begin{figure}[!htb]
%   \centering
%   \caption{}
%   \includegraphics[scale=.5]{.pdf}
%   \label{fig:}
% \end{figure}