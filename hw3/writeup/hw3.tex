\documentclass[11pt, fleqn]{article}

\input{../../utils/header.tex}

% \crefname{figure}{Figure}{Figures}
% \crefname{section}{Section}{Sections}
% \crefname{table}{Table}{Tables}
% \crefname{lstlisting}{Listing}{Listings}

\setlength{\parskip}{12pt} % Sets a blank line in between paragraphs
\setlength\parindent{0pt} % Sets the indent for each paragraph to zero

\begin{document}

\title{Machine Learning (41204-01)\\HW \#3}
\author{Will Clark $\vert$ Matthew DeLio \\
\texttt{will.clark@chicagobooth.edu} $\vert$ \texttt{mdelio@chicagobooth.edu} \\
University of Chicago Booth School of Business}
\date{\today}
\maketitle

\section{Data}

For this exercise, our data set contains the sale price and the observable characteristics for a sample of 20,000 used cars. We randomly sample it to break this data into three subsets: 50\% of data will be our training set that will be used to train/tune our models ($n=10,031$); 25\% will be our validation data set which we will use to evaluate model performance ($n=5,016$); and 25\% will be our test set which we will use to evaluate out-of-sample performance of our best model ($n=5,016$).

We will build a series of models that can predict the selling price of a used car given its observable characteristics. The models and techniques we will use are: (1) regression trees, (2) bagging, (3) random forests, and (4) boosting trees.

\section{Regression Trees}

We begin by fitting a simple regression tree to the data. We use the \texttt{rpart} package on the training data set discussed above. The \texttt{rpart} method returns an object that includes a matrix of the optimal tree prunings. We use this matrix to find the tree complexity parameter that produces the lowest error and prune the tree to this level of complexity. The original tree and pruned tree for a small model (i.e. price on mileage) are depicted in \cref{fig:tree_small,fig:tree_small_prune}, and it is clear that the pruned model has fewer end nodes than the original model.

Because the default options on \texttt{rpart} choose a very simple tree, we lower the minimum split (i.e. smallest number of observations in a node in order for it to be split) from 20 to five, and we lower the complexity parameter from 1/100 to 5/10,000 (which effectively builds more splits into the tree). This produces a more complex model, but one that still does not perform well out-of-sample, especially in comparison to the other algorithms discussed below.

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.49\textwidth}
 \caption{Small Regression Tree}
 \includegraphics[width=\textwidth]{tree_small.pdf}
 \label{fig:tree_small}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
 \caption{Small Regression Tree (Pruned)}
 \includegraphics[width=\textwidth]{tree_small_prune.pdf}
 \label{fig:tree_small_prune}
  \end{subfigure}
 \caption{Comparison of Small Regression Tree Models}
\end{figure}

\section{Bagging}

In this section, we use an aggregate bootstrap technique to predict used car sales price. The basic algorithm is:
\begin{itemize}
\item For a given number of trials $T$:
\begin{itemize}
\item Select a bootstrap sample from the data and fit a large regression tree on this sample (in this case we take large to mean a tree that is not pruned);
\item Use the large tree to make a prediction for expected price;
\end{itemize}
\item Take an average of the predicted price across all trials.
\end{itemize}

Ultimately, as we will see, the bagging algorithm does not perform very well relative to the boosting tree, the random forest, and the LASSO regression. 

\section{Random Forest}

In this section, we try to predict car price with a random forest algorithm. The main difference between the algorithm here and that in the prior section is that for each tree, instead of estimating based on the entire set of covariates, we estimate only on a subset $m$ of covariates. This makes the algorithm train more quickly and introduces another layer of randomness into our predictions. 

We chose a value of $m=3$, which produced the best set of out-of-sample RMSE values. We also chose to stop the algorithm after 250 trees, as the predictive performance (measured by out-of-bag RMSE) fails to improve after this point (see \cref{fig:rf_oob_mse}). In order to speed up performance even more, we can also divide our intended number of trees (250) by the number of processors available (eight, in this case). We then build each small forest on a separate processor, combine the eight forests at the end into one larger forest and use this final combined forest to predict car price.

The random forest algorithm ends up performing very well, nearly beating the best-in-class performance of the boosting tree.

\begin{figure}[!htb]
  \centering
  \caption{Out-of-Bag Mean Square Error by Number of Trees}
  \includegraphics[scale=.5]{rf_oob_mse.pdf}
  \label{fig:rf_oob_mse}
\end{figure}

\section{Boosting Trees}

In this section, we predict the car price using boosting trees.  As discussed in class, boosting trees are subtly different than the other algorithms discussed.  Just as in the random forest model, the end result is a forest of trees; however, the set of trees are fit on the residuals leftover from all previous passes through upstream shrunken trees.  There are three main parameters that we can choose when fitting such a forest:
\begin{itemize}
\item Crush factor ($\lambda)$ - amount to truncate each tree;
\item  Interaction depth (d) - a maximum size (proxy for complexity) of each tree;
\item  \# of trees (B) - the maximum number of trees in the forest.
\end{itemize}

The complexity in choosing these values is that, done properly, we would need to minimize our validation sample's RMSE by optimizing these three parameters simultaneously.  This quickly becomes a difficult convex optimization problem which is outside the scope of this assignment and course.  However, in the interest of choosing some reasonable values, we move forward by fixing two of the three values to some reasonable choice, sweeping the third, and iterating until we find something that approximates the ``best'' model available (or at least what looks like a local minima).

For the remainder of this section we confine ourselves to choosing optimal parameters for a ``large'' model consisting of all covariates.  First we set the nominal parameters to $\lambda=0.11$, $d=16$, and $B=50$, then one of these values is swept to see its effect on the RMSE; \cref{fig:gbm_shrink,fig:gbm_indepth,fig:gbm_ntree} shows the results of these sweeps (each averaged 100 times).  We note a large amount of sample noise, even with a relatively large averaging size, and therefore chose to employ a technique similar to the model selection rules present in the gamma lasso model.

To avoid over-fitting our data (which introduces variance), we first find the parameter choice that minimizes the validation RMSE and then calculate the variance it exhibits over all 100 runs.  Next we find the parameter choice that yields an RMSE 1-$\sigma$ worse that this minimum.  In the figures, the minimum is shown as the dotted line in each of these figures with the solid line indicating the 1se rule.  In each case this chooses a simpler model that is less prone to overfitting, yet still yields exceptional RMSE performance.

Our optimal choice of parameters was found after many iterations from an initial set to be the nominal ones used to produce \cref{fig:gbm_shrink,fig:gbm_indepth,fig:gbm_ntree}.  Therefore, these parameters will be used to fit our final model.

As we will see in the final section, even though this algorithm is difficult to tune, it has the best RMSE performance.

% boost.ntree = 50
% boost.indepth = 16
% boost.shrink = 0.11

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.44\textwidth}
    \caption{Crush Factor $\lambda$}
    \includegraphics[width=\textwidth]{gbm_shrink.pdf}
    \label{fig:gbm_shrink}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.44\textwidth}
    \caption{Interaction Depth}
    \includegraphics[width=\textwidth]{gbm_indepth.pdf}
    \label{fig:gbm_indepth}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.44\textwidth}
    \caption{\# number of Trees Paramter}
    \includegraphics[width=\textwidth]{gbm_ntree.pdf}
    \label{fig:gbm_ntree}
  \end{subfigure}
  \caption{Optimal Tuning Parameters of Boosting Trees}
\end{figure}

\section{LASSO Regression}
In this section we use the linear regression to create a model we can compare to the trees we have been developing.  Do do this, we turn to the gamma-lasso regression.

We recognize pretty quickly that without adding interaction terms, the linear regression will perform terribly, so to make the comparison a little fairer, we add a single-layer of interactions (allowing each covariate to have an effect on another).  Also, because we found that the histogram of prices was not as ``normally'' distributed as we would have liked, we use a logarithm to transform it a bit.  Finally, we perform model selection using a 5-fold cv-gamma lasso with the ``1se'' selection criteria.  See \cref{fig:lin_reg} for a plot of the lasso run; note that the right-most vertical line in the plot is the ``1se'' $\lambda$.

As it turns out, this model actually holds its own quite well and is a strong contender in the top-3 spot (behind boosting trees and random forests).

\begin{figure}[!htb]
  \centering
  \caption{Trace of Gamma-Lasso Run for Linear Regressive Model}
  \includegraphics[scale=.5]{lin_reg_interaction.pdf}
  \label{fig:lin_reg}
\end{figure}

\section{Comparison and Out-of-Sample Test}

In \cref{tab:rmse_comp}, we list the out-of-sample RMSE of each model for the validate data set. The models denoted by ``large'' are those trained on the entire set of covariates; those denoted by ``small'' are trained only on the mileage series.

The best performing model is the large boosting tree model, followed very closely by the large random forest model and the large LASSO regression model. All three models perform similarly, although we note that the boosting tree model was the most difficult to tune. In a production environment, deciding which model to employ would depend not only on RMSE performance but on computational expense. Because the three models mentioned above all perform so closely, we will look more closely at the out-of-sample RMSE produced by the test set (the unused 25\% of our data-set) to make a final judgement.

The results of this out-of-sample test are shown in \cref{tab:rmse_test}. Two things are clear. The first is that the \textit{true} out-of-sample RMSE for all three models is very close to our expected out-of-sample RMSE (which was based on the validation data set). The second is that the order of performance of the models didn't change: the finely tuned boosting tree still narrowly out-performs the random forest in the true out-of-sample test. However, it is worth noting that the difference between the two models is almost imperceptible.  This suggests that at the margin, we can differentiate what is the ``best'' model by other factors like tractability and computational expense, and out-of-sample performance will not suffer much as a result.

\input{rmse_comp}

\input{rmse_test}

\end{document}

% \input{.tex}

% \begin{figure}
%   \centering
%   \begin{subfigure}[b]{0.49\textwidth}
%     \caption{}
%     \includegraphics[width=\textwidth]{.pdf}
%     \label{fig:}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}[b]{0.49\textwidth}
%     \caption{}
%     \includegraphics[width=\textwidth]{.pdf}
%     \label{fig:}
%   \end{subfigure}
%   \caption{}
% \end{figure}

% \begin{figure}[!htb]
%   \centering
%   \caption{}
%   \includegraphics[scale=.5]{.pdf}
%   \label{fig:}
% \end{figure}

