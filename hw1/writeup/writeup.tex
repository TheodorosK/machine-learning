\documentclass[11pt, fleqn]{article}

\input{../../utils/header.tex}

% \crefname{figure}{Figure}{Figures}
% \crefname{section}{Section}{Sections}
% \crefname{table}{Table}{Tables}
% \crefname{lstlisting}{Listing}{Listings}

\setlength{\parskip}{12pt} % Sets a blank line in between paragraphs
\setlength\parindent{0pt} % Sets the indent for each paragraph to zero

\begin{document}

\title{Machine Learning () HW \#1}
\author{Will Clark \& Matthew DeLio \\
University of Chicago Booth School of Business}
\date{\today}
\maketitle

% Describe the data, show a plot of mileage vs price

\section{Data Description}

The data set for this exercise the sale price and observed mileage for 1000 used cars. We can see in \cref{fig:linear} that the expected inverse relationship between mileage and sale price is borne out by the data (i.e. high-mileage cars are less expensive than low-mileage cars).

\section{Linear Regression Model}\label{sec:linear}

To describe this data, we first estimate a linear model of mileage on price:
\[ p_i = \alpha + \beta m_i + \varepsilon_i \]
where \(p_i\) is the price of car \(i\), \(m_i\) is the observed mileage on car \(i\), \(\alpha\) and \(\beta\) are estimated regression coefficients, and \(\varepsilon_i\) is the residual. We fit a generalized linear model to the data and find that \(\alpha=56359.7\) and \(\beta=-0.35\). That is, a hypothetical used car with no miles would sell for about \$56 thousand, and each additional mile driven lowers a car's expected selling price by \$0.35.

As depicted in \cref{fig:linear}, this model appears to be a weak fit for the data. The distribution of residuals is not normally distributed. The model tends to systematically under-value cars with very low mileage and with very high mileage (see \cref{fig:lin_errors} in the Appendix). 

\begin{figure}[!htb]
  \centering
  \includegraphics[scale=.5]{linear_fit.pdf}
  \caption{Linear Regression of Price on Mileage}
  \label{fig:linear}
\end{figure}

\section{k-Nearest Neighbors Algorithm}

As an alternative to a linear model, we can use a k-nearest neighbors (knn) algorithm. This algorithm will compute an expected price for every given mileage by taking an average of the observed price for the k-number of cars with the closest observed mileage. 

The decision we are faced with is choosing a value for the k-number of nearest neighbors to include in the algorithm. There are many different decision criteria we could use, but we will focus on two: 1) out-of-sample root mean square error (RMSE); and 2) n-fold cross-validation (CV).

\subsection{Out-of-Sample RMSE}\label{sec:oos}

To choose \(k\) using out-of-sample RMSE, we break our data into a training sample (with 900 data points) to train a model, and a test sample (with the remaining 100 points) to estimate the model's goodness of fit. We then estimate a model for all values of \(k \in (2,100)\) and compare their out-of-sample RMSEs, where RMSE is defined by:
\[ \text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} \left( p_i - \hat{p_i} \right)^2} \]

where \(p_i\) is the observed price of car \(i\) and \(\hat{p_i}\) is the predicted price of car \(i\) given its mileage. The RMSE is a measure of how closely a model fits the data; in this case, what we care about is how well the model fits data points that it has not been trained on. We then choose \(k\) that produces the lowest out-of-sample RMSE. The results are depicted in \cref{fig:sweep} (in the red line). We can see that the minimum RMSE occurs at \(k=12\), which is circled in the plot.\footnote{The value of \(k\) selected depends heavily on the hold-out sample, which is selected randomly. We set R's random number generator seed in this example, but letting it vary produces very different values of \(k\) fluctuating around the cross-validated \(k\) shown in next section.}

% use n-fold cross-validation
\subsection{n-fold Cross Validation}\label{sec:cv}

The alternative method is to choose \(k\) via cross validation. In this method, we will measure out-of-sample fit in the same method as in the prior section (via RMSE), but we will average the out-of-sample fit of ten different training sets (i.e. n-fold cross-validation with \(n=10\)). The algorithm, briefly:
\begin{itemize}
\item For all \(k \in (2,100)\):
\begin{itemize}
\item Break the sample into 10 different data sets
\item For all \(n \in (1, 10)\):
\begin{itemize}
\item Hold out the \(n\)-th data set; train a model on the remaining data
\item Calculate the RMSE of the \(n\)-th (held out) data set based on the predicted values of the k-nn model
\end{itemize}
\item Each \(k\) will have \(n\) RMSE values; take the average of all these RMSEs.
\end{itemize}
\item Select the model (\(k\)) with the lowest average RMSE (which we will call cross-validated error).
\end{itemize}

The resulting RMSE for various values of k is shown in \cref{fig:sweep} (the light blue line). The \(k\) that results in the lowest cross-validated error is \(k=40\), which is circled in blue in the figure.\footnote{As a comparison, using the \texttt{kknn} package's built-in algorithm for hold-one-out cross-validation selects a model with \(k=41\), so our result is close to optimal.}

\begin{figure}[!htb]
  \centering
  \includegraphics[scale=.5]{sweep_kknn.pdf}
  \caption{CV and OOS RMSE for Various Values of k in kNN}
  \label{fig:sweep}
\end{figure}

\section{Model Predictions}
Before making any predictions it is worth comparing the linear model found in \cref{sec:linear} to the ones developed in \cref{sec:oos,sec:cv}.  See \cref{fig:models} for a visual comparison of how the different algorithms track the given cars data.  For completeness, the figure shows a relatively low value of $k$ (the one indicated in \cref{sec:oos}), one with $k=40$ (prescribed through our cross-validation method in \cref{sec:cv}), and finally one with $k=500$ (a arbitrary and unreasonably large choice).  Here the Bias-Variance tradeoff is clearly illustrated.  With small values of $k$ (e.g.; $k=12$), kNN tracks the data closely but also over-tracks tracks the variance.  As $k$ is increased to unreasonably large values it tends to ignore the variance and introduces a large amount of bias in the predictions (e.g.; $k=500$).  The sweet spot appears to be, at, roughly $k=40$; a point where the model still tracks the  data-set well, but does not amplify the variance.

Indeed, the results can be seen in \cref{tab:rmse_compare} by examining the RMSE across all of these algorithms (using the same training/test sample as in \cref{sec:oos}).  Here it is evident that the RMSE is worse for the Linear and $k=500

\input{rmse_compare.tex}

% Predict car price with cross-validated knn and with linear model

\begin{figure}[!htb]
  \centering
  \includegraphics[scale=.5]{pred_models.pdf}
  \caption{Comparison of Models Created by the Linear and kNN}
  \label{fig:models}
\end{figure}


\begin{figure}[!htb]
  \centering
  \includegraphics[scale=.5]{predict.pdf}
  \caption{Linear \& kNN Predictions}
  \label{fig:predict}
\end{figure}

\clearpage
\section{Appendix}

\begin{figure}[!htb]
  \centering
  \includegraphics[scale=.5]{lin_errors.pdf}
  \caption{Distribution of Residuals in Linear Model}
  \label{fig:lin_errors}
\end{figure}

\end{document}

% \input{.tex}

% \begin{figure}
%   \centering
%   \begin{subfigure}[b]{0.49\textwidth}
%     \includegraphics[width=\textwidth]{.pdf}
%     \caption{}
%     \label{fig:}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}[b]{0.49\textwidth}
%     \includegraphics[width=\textwidth]{.pdf}
%     \caption{}
%     \label{fig:}
%   \end{subfigure}
%   \caption{}
% \end{figure}

% \begin{figure}[!htb]
%   \centering
%   \includegraphics[scale=.5]{.pdf}
%   \caption{}
%   \label{fig:}
% \end{figure}

